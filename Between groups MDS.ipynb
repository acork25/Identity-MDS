{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38774458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a54731",
   "metadata": {},
   "source": [
     "The files we are importing here should be Reddit data taken from GoogleBig Query which has been run through the LIWC software. The data should be formatted with the reddit metadata (i.e. post content, author, subreddit, url) in the first few columns, followed by the LIWC variables (i.e., WC, WPS, they, tentative etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_data = pd.read_csv() #filepath to Reddit comments which have been through the LIWC software\n",
    "comments_data = pd.read_csv() #filepath to Reddit posts which have been through the LIWC software\n",
    "# These need to be treated separately as they contain different headings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452a109",
   "metadata": {},
   "source": [
    "If these have come straight from the LIWC software, you may need to rename the headings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af37fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f275a3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_data.rename(columns={'B':'author','C':'body','D':'subreddit'}, inplace=True)\n",
    "posts_data.rename(columns={'B':'author','C':'title','D':'selftext', 'E': 'subreddit','F':'url'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0b864",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8a29e",
   "metadata": {},
   "source": [
    "Here we remove posts and comments that are not suitable for the analysis. This includes posts/comments which: \n",
    "(i) have been removed or deleted, \n",
    "(ii) are from authors who have been removed or deleted, \n",
    "(iii) contain strings including the word 'bot' as bots often identify themselves\n",
    "(iv)  contain urls\n",
    "(v)  contain words under the set word count limit (often 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceb5933",
   "metadata": {},
   "source": [
    "Note: In the LIWC software there is a variable titled 'body', you may need to rename this variable so as not to confuse the LIWC variable with the 'body' column referring to the body of text in the comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b87129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_posts(df, wordcount):\n",
    "    print('Posts starting:', df.shape)\n",
    "    df= df.loc[df['selftext']!='[removed]']\n",
    "    df= df.loc[df['selftext']!='[deleted]']\n",
    "    print('selftext removed/deleted: ', len(df))\n",
    "    df= df.loc[df['author']!='[removed]']\n",
    "    df= df.loc[df['author']!='[deleted]']\n",
    "    print('author removed/deleted: ', len(df))\n",
    "    df=df[~df.selftext.str.contains(' bot ', na=False)]\n",
    "    print('text with no bots: ', len(df))\n",
    "    df=df[~df.author.str.contains('AutoModerator', na=False)]\n",
    "    df=df[~df.author.str.contains('bot', na=False)]\n",
    "    print('author no bots: ', len(df))\n",
    "    print(df.shape)\n",
    "    df = df[df['url'].str.contains(\"reddit\")]\n",
    "    print('URLs removed: ', len(df))\n",
    "    df = df.loc[df['WC']>wordcount]\n",
    "    print('WC removed: ', len(df))\n",
    "    return df\n",
    "\n",
    "def clean_comments(df, wordcount):\n",
    "    print('Comments starting:', df.shape)\n",
    "    df= df.loc[df['body']!='[removed]']\n",
    "    df= df.loc[df['body']!='[deleted]']\n",
    "    print('body removed/delted: ', len(df))\n",
    "    df= df.loc[df['author']!='[removed]']\n",
    "    df= df.loc[df['author']!='[deleted]']\n",
    "    print('author removed/deleted: ', len(df))\n",
    "    df=df[~df.body.str.contains(' bot ', na=False)]\n",
    "    print('text with no bots: ', len(df))\n",
    "    df=df[~df.author.str.contains('AutoModerator', na=False)]\n",
    "    df=df[~df.author.str.contains('bot', na=False)]\n",
    "    print('author no bots: ', len(df))\n",
    "    df = df.loc[df['WC'] > wordcount]\n",
    "    print('WC removed: ', len(df))\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleancomments = clean_comments(comments_data, 49)\n",
    "cleanposts = clean_posts(posts_data, 49)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027aa179",
   "metadata": {},
   "source": [
    "Here, we drop any duplicate posts/comments in each forum and find out how many posts/comments we have remaining in each forum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f80ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "forums = cleanposts.subreddit.unique()\n",
    "\n",
    "post_lengths=[]\n",
    "comment_lengths=[]\n",
    "\n",
    "for forum in forums:\n",
    "    cleanposts.drop_duplicates(['selftext','title'], inplace=True)\n",
    "    cleancomments.drop_duplicates('body', inplace=True)\n",
    "    posts_filtered = cleanposts.loc[cleanposts['subreddit']==forum]\n",
    "    print(forum, 'posts : ', len(posts_filtered))\n",
    "    post_lengths.append(len(posts_filtered))\n",
    "    comments_filtered = cleancomments.loc[cleancomments['subreddit']==forum]\n",
    "    print(forum, 'comments : ', len(comments_filtered))\n",
    "    comment_lengths.append(len(comments_filtered))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c7b9da",
   "metadata": {},
   "source": [
    "Next, we take a sample of data from each forum which matches the forum with the lowest number of posts/comments. That is, if one forum has only 10,000 comments, we will take 10,000 comments from all forums in order to ensure equal sample sizes. This is important as it impacts how we interpret our later AUCs. Please note that there are also other methods available to ensure equal sample sizes such as creating synthetic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalsample=pd.DataFrame()\n",
    "\n",
    "allstyle = ['WPS', 'Sixltr','i', 'we', 'you', 'shehe', 'they', 'ipron',\n",
    "       'article', 'prep', 'auxverb', 'adverb', 'conj', 'negate', 'verb',\n",
    "       'adj', 'compare', 'interrog', 'number', 'quant', 'affect','insight', 'cause',\n",
    "       'discrep', 'tentat', 'certain', 'differ','see', 'hear',\n",
    "       'feel', 'focuspast', 'focuspresent', 'focusfuture', 'motion', 'space',\n",
    "       'time','swear', 'netspeak', 'assent', 'nonflu', 'filler', 'subreddit']\n",
    "\n",
    "for forum in forums:\n",
    "    posts_filtered = cleanposts.loc[cleanposts['subreddit']==forum]\n",
    "    print(forum, 'posts : ', len(posts_filtered))\n",
    "    posts_sample = posts_filtered.sample(min(post_lengths))\n",
    "    comments_filtered = cleancomments.loc[cleancomments['subreddit']==forum]\n",
    "    print(forum, 'comments : ', len(comments_filtered))\n",
    "    comments_sample = comments_filtered.sample(min(comment_lengths))\n",
    "    sampledf = pd.concat([comments_sample[allstyle], posts_sample[allstyle]]) #Keep only columns relevant to analysis\n",
    "    totalsample = pd.concat([totalsample, sampledf])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b68e4",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0772cb8",
   "metadata": {},
   "source": [
    "First step is to create the test and training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a245d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a4a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = pd.DataFrame()\n",
    "traindata = pd.DataFrame()\n",
    "\n",
    "test_train_size = (min(post_lengths) + min(comment_lengths))//2\n",
    "print (test_train_size)\n",
    "\n",
    "for forum in forums:\n",
    "    filtered = totalsample.loc[totalsample['subreddit']==forum]\n",
    "    print(forum)\n",
    "    test = filtered.sample(int(test_train_size))\n",
    "    train = filtered[~filtered.isin(test)].dropna()\n",
    "    print('train = ',len(train), 'test=',(len(test)))\n",
    "    testdata = pd.concat([testdata, test])\n",
    "    traindata = pd.concat ([traindata, train])\n",
    "\n",
    "print(len(traindata), len(testdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9b9d54",
   "metadata": {},
   "source": [
    "Next, we run the Extra Trees classifier. Here we use 5 fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445572f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(identity1, identity2):\n",
    "    train1 = traindata.loc[traindata['subreddit']==identity1].drop(['subreddit'], axis=1)\n",
    "    train2 = traindata.loc[traindata['subreddit']==identity2].drop(['subreddit'], axis=1)\n",
    "    test1 = testdata.loc[testdata['subreddit']==identity1].drop(['subreddit'], axis=1)\n",
    "    test2 = testdata.loc[testdata['subreddit']==identity2].drop(['subreddit'], axis=1)\n",
    "    \n",
    "    print(len(train1), len(train2), len(test1),len(test2))\n",
    "    t = np.ones((train1.shape[0]+train2.shape[0]))\n",
    "    t[:train1.shape[0]] = 0\n",
    "\n",
    "    X = np.vstack((train1.values, \n",
    "            train2.values))\n",
    "    X.shape, train1.shape, train2.shape\n",
    "    \n",
    "    t1 = np.ones((test1.shape[0]+test2.shape[0]))\n",
    "    t1[:test1.shape[0]] = 0\n",
    "\n",
    "    t2 = np.vstack((test1.values, \n",
    "            test2.values))\n",
    "    t2.shape, test1.shape, test2.shape\n",
    "\n",
    "    et = ExtraTreesClassifier(n_estimators=300, max_depth=None, \n",
    "                              min_samples_split=2, random_state=8, \n",
    "                              n_jobs=-1)\n",
    "\n",
    "    scores = ['accuracy', 'roc_auc']\n",
    "\n",
    "    for score in scores:\n",
    "        result = cross_val_score(et, X, t, cv=5, scoring=score)\n",
    "        print(\"ExtraTrees for %s\" % score)\n",
    "        print(\"%0.3f (+/-%0.03f)\" % (np.mean(result), np.std(result)*2))\n",
    "\n",
    "    print(X.shape)\n",
    "    et.fit(X, t)\n",
    "    s_train = et.predict_proba(X)\n",
    "\n",
    "    y_true, y_pred = t1, et.predict(t2)\n",
    "    s = et.predict_proba(t2)[:,1]\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, s)\n",
    "    auc = roc_auc_score(y_true, s)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print('ExtraTrees  AUC %g   Accuracy %g' % (auc, acc))\n",
    "    return auc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysisdf = pd.DataFrame(columns=forums, index=forums)\n",
    "x=0\n",
    "\n",
    "while x < (len(forums)+1): #Loop through the number of forums\n",
    "    for forum in forums:\n",
    "        print(forum, forums[x])\n",
    "        auc, acc = run_classifier(forum, forums[x])\n",
    "        analysisdf.at[forum, forums[x]] = auc\n",
    "    x+=1\n",
    "\n",
    "print(analysisdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0fffa3",
   "metadata": {},
   "source": [
    "Now we've got the AUCs for each pair of identities, we can run the multidimensional scaling analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7a0c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = analysisdf.values\n",
    "D=np.array(D, dtype=float)\n",
    "plt.imshow(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b668ac0",
   "metadata": {},
   "source": [
    "Here, we should see a symmetrical heat map of the AUCs from the different identity pairs. To check they are symmetrical, run the code below. It should return True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_symmetric(a, tol=1e-8):\n",
    "    return np.all(np.abs(a-a.T) < tol)\n",
    "check_symmetric(D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa0281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn import preprocessing\n",
    "from mpl_toolkits import mplot3d\n",
    "    \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc0ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MDS(n_components=2, dissimilarity='precomputed', random_state=1)\n",
    "mds_out = model.fit_transform(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2015b588",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.set_aspect('equal')\n",
    "plot = ax.scatter(mds_out[:,0], mds_out[:,1], s=20, vmin=-0.6, vmax=0.6)\n",
    "\n",
    "\n",
    "for i, txt in enumerate(analysisdf.columns.values):\n",
    "        ax.annotate(txt, xy=(mds_out[:,0][i], mds_out[:,1][i]), xytext=(10, 10),\n",
    "                    fontsize=15, va='top',\n",
    "                    xycoords='data', textcoords='offset points')\n",
    "\n",
    "\n",
    "# Ticks and labels\n",
    "\n",
    "\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tick_params(axis='both', labelsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2a30ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MDS(n_components=3, dissimilarity='precomputed', random_state=1)\n",
    "mds_out = model.fit_transform(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter3D(mds_out[:,0], mds_out[:,1], mds_out[:,2], s=100, c=mds_out[:,2])\n",
    "\n",
    "ax.view_init(azim=20, elev=30)\n",
    "\n",
    "for i, txt in enumerate(analysisdf.columns.values):\n",
    "    ax.text(mds_out[:,0][i], mds_out[:,1][i], mds_out[:,2][i], txt, fontsize=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4e42d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
